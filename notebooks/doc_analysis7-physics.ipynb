{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#generic scifeed document analyzer\n",
    "#read in the discipline specific config file and it will load the data and go from there.\n",
    "\n",
    "from pattern.web import Newsfeed\n",
    "from pattern.web import cache\n",
    "from pattern.en import parse, pprint, tag\n",
    "from pattern.web import download, plaintext\n",
    "import numpy as np\n",
    "import nltk.stem\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_docs(path, name):\n",
    "    filename = path+name+\".p\"\n",
    "    fileobj = open(filename, \"rb\")\n",
    "    z = fileobj.read()\n",
    "    lst = pickle.loads(str(z))\n",
    "    titles = []\n",
    "    sitenames = []\n",
    "    abstracts = []\n",
    "    for i in range(0, len(lst)):\n",
    "        titles.extend([lst[i][0]])\n",
    "        sitenames.extend([lst[i][1]])\n",
    "        abstracts.extend([lst[i][2]])\n",
    "        \n",
    "    print(\"done loading \"+filename)\n",
    "    return abstracts, sitenames, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_config(subj, basepath):\n",
    "    docpath =basepath+ \"/config_\"+subj+\".json\"\n",
    "    with open(docpath, 'rb') as f:\n",
    "        doc = f.read() \n",
    "    z =json.loads(doc)\n",
    "    subject = z['subject']\n",
    "    loadset = z['loadset']\n",
    "    subtopics = []\n",
    "    for w in z['supertopics']:\n",
    "        subtopics.extend([(w[0], w[1])])\n",
    "    return subject, loadset, subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading /home/jovyan/work/phy_train.p\n"
     ]
    }
   ],
   "source": [
    "titles, sitenames, disp_title = load_docs(\"/home/jovyan/work/\", \n",
    "                                     \"phy_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "subj, loadset, supertopics = read_config(\"phy\", \"/home/jovyan/work\")\n",
    "#special case to eliminate other-phy\n",
    "supertopics = supertopics[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Astro', u'GenRelQGr', u'Cond Matter', u'high eng', u'MathPh', u'quantum', u'ed-physics']\n"
     ]
    }
   ],
   "source": [
    "supertopic_names = [x[0] for x in supertopics]\n",
    "print supertopic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora, models, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildVectorizer(bio):\n",
    "    nounlist = []\n",
    "    for doc in bio:\n",
    "        st = \"\"\n",
    "        for (word, pos) in tag(doc):\n",
    "            if pos in [\"JJ\", \"NNS\", \"NN\", \"NNP\"]:\n",
    "                st = st+word+\" \"\n",
    "            else:\n",
    "                if st!= \"\":\n",
    "                    st = st[0:-1]+\" \"\n",
    "                    #print \"got one\"\n",
    "        nounlist.extend([st])\n",
    "    sciencestopwords = set([u'model','according', 'data', u'models', 'function', 'properties', 'approach', 'parameters', \n",
    "                    'systems', 'number', 'order', u'data', 'analysis', u'information', u'journal',\n",
    "                    'results','using','research', 'consumers', 'scientists', 'model', 'models', 'journal',\n",
    "                    'researchers','paper','new','study','time','case', 'simulation', u'simulation', 'equation',\n",
    "                    'based','years','better', 'theory', 'particular','many','due','much','set', 'studies', 'systems',\n",
    "                    'simple', 'example','work','non','experiments', 'large', 'small', 'experiment', u'experiments',\n",
    "                    'provide', 'analysis', 'problem', 'method', 'used', 'methods'])\n",
    "    #now doing the new vectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    english = nltk.corpus.stopwords.words('english')\n",
    "    newstop = english+list(sciencestopwords) \n",
    "    vectorizer = TfidfVectorizer(min_df=1, max_df=.5, stop_words=newstop, decode_error='ignore')\n",
    "    X = vectorizer.fit_transform(nounlist)\n",
    "    Xinv = vectorizer.inverse_transform(X)\n",
    "        #X is a sparse matrix of docs x vocab size (7638). \n",
    "    #so X[doc_num] is the sparse vector of its words. \n",
    "    #the ||X[doc_num]|| = 1 there are 7638 unique words and 755 docs. with a total number of 38888 non-zeros.\n",
    "    #Xinv[doc_num] is the list of words in the doc.\n",
    "     \n",
    "    return nounlist, vectorizer, X, Xinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nounlist, vectorizer, X, Xinv = buildVectorizer(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "texts = [ item.tolist() for item in Xinv]\n",
    "print len(texts)\n",
    "print len(nounlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#returns a list consiting of tuples\n",
    "#   (subtopic_name, [list of title-numbers in that subtopic])\n",
    "def make_topic_lists_sets(nounlist, disp_title, supertopics):\n",
    "    topic_lists = []\n",
    "    for topic in supertopics:\n",
    "        #print topic[0]\n",
    "        subtop_list = []\n",
    "        for i in range(0,len(nounlist)):\n",
    "            t = disp_title[i]\n",
    "            for j in topic[1]:\n",
    "                x = t.find(j) \n",
    "                if x  > 0:\n",
    "                    subtop_list.extend([i])\n",
    "        topic_lists.extend([(topic[0], subtop_list)])\n",
    "        #print subtop_list\n",
    "    return topic_lists #returns the list of tuples for the training sets\n",
    "    \n",
    "\n",
    "# for each subtopic\n",
    "#   (subtopic name, training set items, list of the ARXiV sub areas for this supertopic )\n",
    "def fillTopicTables(nounlist, disp_title, supertopics ):\n",
    "    toplsts = make_topic_lists_sets(nounlist, disp_title, supertopics)\n",
    "    super_topics = []   \n",
    "    for i in range(0,len(toplsts)):\n",
    "        topl = toplsts[i]\n",
    "        area = topl[0]\n",
    "        items = topl[1]\n",
    "        z = int(3.0*len(items)/4)\n",
    "        tupletitles = []\n",
    "        tuplenums = []\n",
    "        for r in range(0,z):\n",
    "            w = int(random.random()*len(items))\n",
    "            tupletitles.extend([nounlist[items[w]]])\n",
    "            tuplenums.extend([items[w]])\n",
    "        tup = (area, tupletitles, supertopics[i][1])\n",
    "        super_topics.extend([tup])\n",
    "    \n",
    "    for top in super_topics:\n",
    "        print top[0] + \" \"+ str(len(top[1]))+ \" \" + str(top[2])\n",
    "\n",
    "    return super_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def makeInvertedTrainingList(super_topics):\n",
    "    #create list of all training set items\n",
    "    #  (doc, docno, subtopicname, subtopic-index)\n",
    "    lis = []\n",
    "    n = 0\n",
    "    for top in super_topics:\n",
    "        items = top[1]\n",
    "        for i in range(0, len(items)):\n",
    "            lis.extend([(items[i], top[0], n)])\n",
    "        n = n+1\n",
    "    return lis\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_centroid(items, vectorizer):\n",
    "    stmt = items[0]\n",
    "    #stmt = titles[items[0]]\n",
    "    count = len(items)\n",
    "    #print stmt\n",
    "    #print count\n",
    "    vec = vectorizer.transform([stmt])[0]\n",
    "    for i in range(1,count):\n",
    "        stmt = items[i]\n",
    "        vec2 = vectorizer.transform([stmt])[0]\n",
    "        vec = vec+vec2\n",
    "    z = np.linalg.norm(vec.toarray())\n",
    "    return vec/z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#the X and titles used here should be for the training list\n",
    "def makeguess(statement, km, vectorizer, lsi, dictionary, index_lsi, ldamodel, index_lda, X, titles):\n",
    "    new_title_vec = vectorizer.transform([statement])\n",
    "    new_title_label = km.predict(new_title_vec)\n",
    "    similar_indicies = (km.labels_==new_title_label).nonzero()[0]\n",
    "    similar = compdist(new_title_vec, similar_indicies, X, titles)\n",
    "    kmeans_items = list(x[1] for x in similar)\n",
    "\n",
    "    #now for lsi items\n",
    "    new_title_vec_lsi = dictionary.doc2bow(statement.lower().split())\n",
    "    new_title_lsi = lsi[new_title_vec_lsi]\n",
    "    sims = index_lsi[new_title_lsi] # perform a similarity query against the corpus\n",
    "    simlist = list(enumerate(sims))\n",
    "    topten = sorted(simlist, key = lambda x: x[1], reverse = True)[0:30]\n",
    "    lsi_items = list(x[0] for x in topten)\n",
    "\n",
    "    #now do lda\n",
    "    new_title_vec_lda = dictionary.doc2bow(statement.lower().split())\n",
    "    new_title_lda = ldamodel[new_title_vec_lda]\n",
    "    sims = index_lda[new_title_lda] # perform a similarity query against the corpus\n",
    "    simlist = list(enumerate(sims))\n",
    "    topten = sorted(simlist, key = lambda x: x[1], reverse = True)[0:30]\n",
    "    lda_items = list(x[0] for x in topten)\n",
    "\n",
    "    \n",
    "    dist_lsi = compdist(new_title_vec, lsi_items, X, titles)\n",
    "    #dist_lsi = []\n",
    "    dist_km = compdist(new_title_vec, kmeans_items, X, titles)\n",
    "    #dist_km = []\n",
    "    #dist_lda = compdist(new_title_vec, lda_items, X, titles)\n",
    "    dist_lda = []\n",
    "    s = dist_lda + dist_km + dist_lsi\n",
    "    #print s\n",
    "    d1 = sorted(s, reverse=True)\n",
    "    d = [x for x in d1 if x[0] > 0.00]\n",
    "\n",
    "    notdups = []\n",
    "    for x in d:\n",
    "        if x not in notdups:\n",
    "            notdups.extend([x])\n",
    "    return notdups\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def big_build_analizers( subj):    \n",
    "    #subject, loadset, basepath, supertopics =read_config(doc_service, subj)\n",
    "    # for pushing to blob storage set basepath = \"\"\n",
    "    subject=subj\n",
    "    basepath = \"/home/jovyan/work/celery/\"\n",
    "    cvectpath = basepath+\"count_vectorizer-\"+subject+\".p\"\n",
    "    tfidftranpath = basepath+\"tfidf_transformer-\"+subject+\".p\"\n",
    "    RFpath = basepath+\"random_forest-\"+subject+\".p\"\n",
    "    namspath = basepath+\"supertopic_names-\"+subject+\".p\"\n",
    "    GBpath = basepath+\"gradientboosting-\"+subject+\".p\"\n",
    "    vectpath = basepath+\"vectorizer-\"+subject+\".p\"\n",
    "    lsimodpath = basepath+\"lsimod-\"+subject+\".p\"\n",
    "    lsiindpath = basepath+\"lsiind-\"+subject+\".p\"\n",
    "    ldamodpath = basepath+\"ldamod-\"+subject+\".p\"\n",
    "    ldaindpath = basepath+\"ldaind-\"+subject+\".p\"\n",
    "    kmpath =  basepath+\"km-\"+subject+\".p\"\n",
    "    ncentpath = basepath+\"ncent-\"+subject+\".p\"\n",
    "    Xpath = basepath+\"Xtrain-\"+subject+\".p\"\n",
    "    trainsetpath = basepath+\"Tset-\"+subject+\".p\"\n",
    "    dictpath = basepath+\"Dict-\"+subject+\".p\"\n",
    "    \n",
    "    supertopic_names = [ x[0] for x in supertopics]\n",
    "    fo = open(namspath,'wb') \n",
    "    pickle.dump(supertopic_names,fo)\n",
    "    fo.close()\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", namspath, pickled)\n",
    "    \n",
    "    #titles, sitenames, disp_title = load_data2(loadset)\n",
    "    #print \"data loaded\"\n",
    "    #create a version of the docs \"nounlist\" where each item is filtered\n",
    "    #through the stoplist.  Vectorizer is built from that and Xinv is a list\n",
    "    #where each element is the an array of the words in that doc.\n",
    "    #nounlist, vectorizer, X, Xinv = buildVectorizer(titles)\n",
    "    dictwords = [ item.tolist() for item in Xinv]\n",
    "    dictionary = corpora.Dictionary(dictwords)\n",
    "    print(dictionary)\n",
    "    print \"dictionary built\"\n",
    "    dictcorpus = [dictionary.doc2bow(text) for text in dictwords]\n",
    "    tfidf = models.TfidfModel(dictcorpus)\n",
    "\n",
    "    \n",
    "    #create training set\n",
    "    print \"creating training set\"\n",
    "    trainingSets = fillTopicTables(nounlist, disp_title, supertopics)\n",
    "    traininglist = makeInvertedTrainingList(trainingSets)\n",
    "    traindocs = [tex[0] for tex in traininglist]\n",
    "    trainlable = [tex[1] for tex in traininglist]\n",
    "    traintarget = [tex[2] for tex in traininglist]\n",
    "    corpus = [dictionary.doc2bow(text.lower().split()) for text in traindocs]\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    print len(traindocs)\n",
    "    \n",
    "    #create lsi\n",
    "    print \"creating lsi model\"\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) # initialize an LSI transformation\n",
    "    corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "    index_lsi = similarities.MatrixSimilarity(corpus_lsi)\n",
    "\n",
    "    #create lda\n",
    "    print \"creating lda model\"\n",
    "    lda = models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes = 10, iterations = 500) # initialize an LSI transformation\n",
    "    corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "    index_lda = similarities.MatrixSimilarity(corpus_lda)\n",
    "    \n",
    "    #create km for full list of documents \n",
    "    num_clusters = 10\n",
    "    km = KMeans(n_clusters=num_clusters, init='random', n_init=1, verbose=1, tol=.00001)\n",
    "    Xtrain = vectorizer.fit_transform(traindocs)\n",
    "    Xinvtrain = vectorizer.inverse_transform(Xtrain)\n",
    "    print Xtrain.shape\n",
    "    print Xtrain[1].shape\n",
    "    km.fit(Xtrain)\n",
    "    #print Xtrain\n",
    "    print \"k-means analizer built\"\n",
    "    #pickled = pickle.dumps(Xtrain)\n",
    "    fo = open(Xpath,'wb') \n",
    "    pickle.dump(Xtrain,fo)\n",
    "    fo.close()\n",
    "    \n",
    "    fo = open(trainsetpath,'wb') \n",
    "    pickle.dump(trainingSets,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(trainingSets)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", trainsetpath, pickled)\n",
    "   \n",
    "    #dumping km, lda, lsi\n",
    "    fo = open(kmpath,'wb') \n",
    "    pickle.dump(km,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(km)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", kmpath, pickled)\n",
    "    \n",
    "    fo = open(lsimodpath,'wb') \n",
    "    pickle.dump(lsi,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(lsi)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", lsimodpath, pickled)\n",
    "    \n",
    "    fo = open(lsiindpath,'wb') \n",
    "    pickle.dump(index_lsi,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(index_lsi)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", lsiindpath, pickled)\n",
    "    \n",
    "    fo = open(ldamodpath,'wb') \n",
    "    pickle.dump(lda,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(lda)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", ldamodpath, pickled)\n",
    "    \n",
    "    \n",
    "    fo = open(ldaindpath,'wb') \n",
    "    pickle.dump(index_lda,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(index_lda)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", ldaindpath, pickled)\n",
    "    \n",
    "    fo = open(vectpath,'wb') \n",
    "    pickle.dump(vectorizer,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(vectorizer)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", vectpath, pickled)\n",
    "    #pickle.dump( vectorizer, open( vectpath, \"wb\" ) )\n",
    "    \n",
    "    #here is where we create the new centroid list\n",
    "    new_centroids = []\n",
    "    for ts in trainingSets:\n",
    "        print \"computing centroid for \"+ ts[0]\n",
    "        cent = compute_centroid(ts[1], vectorizer)\n",
    "        new_centroids.extend([cent])\n",
    "    print \"dumping new centroids\"\n",
    "    \n",
    "    fo = open(ncentpath,'wb') \n",
    "    pickle.dump(new_centroids,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(new_centroids)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", ncentpath, pickled)\n",
    "       \n",
    "    \n",
    "    training_set_data = np.array(traindocs)\n",
    "    training_set_target = traintarget\n",
    "    print \"total training set size = \"+ str(len(training_set_data))\n",
    "    \n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(training_set_data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    clfrf = RandomForestClassifier(n_estimators = 100)\n",
    "    clfrf.fit(X_train_tfidf, training_set_target)\n",
    "    \n",
    "    print \"dumping vectorizer, transformer and clfrf\"\n",
    "    fo = open(cvectpath,'wb') \n",
    "    pickle.dump(count_vect,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(count_vect)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", cvectpath, pickled)\n",
    "    #pickle.dump( count_vect, open( cvectpath, \"wb\" ) )\n",
    "    \n",
    "    fo = open(tfidftranpath,'wb') \n",
    "    pickle.dump(tfidf_transformer,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(tfidf_transformer)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\",  tfidftranpath, pickled)\n",
    "    #pickle.dump( tfidf_transformer, open( tfidftranpath, \"wb\" ) )\n",
    "    \n",
    "    fo = open(RFpath,'wb') \n",
    "    pickle.dump(clfrf,fo)\n",
    "    fo.close()\n",
    "    #pickled = pickle.dumps(clfrf)\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", RFpath, pickled)\n",
    "    #pickle.dump( clfrf, open( RFpath, \"wb\" ) )\n",
    "    fo = open(dictpath,'wb') \n",
    "    pickle.dump(dictionary,fo)\n",
    "    fo.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print len(nounlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disp_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11908 unique tokens: [u'writings', u'testbeds', u'rovelli', u'yellow', u'four']...)\n",
      "dictionary built\n",
      "creating training set\n",
      "Astro 550 [u'astro-ph.CO', u'astro-ph.EP', u'astro-ph.GA', u'astro-ph.HE', u'astro-ph.IM', u'physics.space-ph', u'astro-ph.SR']\n",
      "GenRelQGr 333 [u'gr-qc']\n",
      "Cond Matter 105 [u'cond-mat.other', u'cond-mat.mes-hall', u'cond-mat.quant-gas', u'cond-mat.soft', u'cond-mat.stat-mech', u'physics.plasm-ph', u'cond-mat.str-el', u'cond-mat.mtrl-sci', u'cond-mat.supr-con']\n",
      "high eng 221 [u'hep-ex', u'hep-ph', u'hep-th', u'nucl-th', u'physics.acc-ph', u'physics.ao-ph', u'physics.atm-clus', u'physics.atom-ph']\n",
      "MathPh 105 [u'math-ph', u'physics.comp-ph', u'physics.data-an', u'physics.flu-dyn']\n",
      "quantum 59 [u'quant-ph']\n",
      "ed-physics 121 [u'physics.pop-ph', u'physics.soc-ph', u'physics.hist-ph', u'physics.ed-ph', u'physics.ins-det', u'physics.gen-ph']\n",
      "1494\n",
      "creating lsi model\n",
      "creating lda model\n",
      "(1494, 8685)\n",
      "(1, 8685)\n",
      "Initialization complete\n",
      "Iteration  0, inertia 2755.161\n",
      "Iteration  1, inertia 1430.028\n",
      "Iteration  2, inertia 1425.274\n",
      "Iteration  3, inertia 1423.519\n",
      "Iteration  4, inertia 1422.442\n",
      "Iteration  5, inertia 1421.595\n",
      "Iteration  6, inertia 1420.659\n",
      "Iteration  7, inertia 1419.739\n",
      "Iteration  8, inertia 1418.941\n",
      "Iteration  9, inertia 1418.558\n",
      "Iteration 10, inertia 1418.264\n",
      "Iteration 11, inertia 1418.123\n",
      "Iteration 12, inertia 1418.045\n",
      "Iteration 13, inertia 1417.997\n",
      "Iteration 14, inertia 1417.985\n",
      "Iteration 15, inertia 1417.973\n",
      "Converged at iteration 15: center shift 0.000000e+00 within tolerance 1.130350e-09\n",
      "k-means analizer built\n",
      "computing centroid for Astro\n",
      "computing centroid for GenRelQGr\n",
      "computing centroid for Cond Matter\n",
      "computing centroid for high eng\n",
      "computing centroid for MathPh\n",
      "computing centroid for quantum\n",
      "computing centroid for ed-physics\n",
      "dumping new centroids\n",
      "total training set size = 1494\n",
      "dumping vectorizer, transformer and clfrf\n"
     ]
    }
   ],
   "source": [
    "big_build_analizers( \"phy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#for an item in the titles list compare its nonlist vector to the list of centroid and return \n",
    "#the sorted list (closest first)\n",
    "def cosdist(vectorizer, itemno, centroids, nounlist):\n",
    "    new_title_vec = vectorizer.transform([nounlist[itemno]])\n",
    "    #new_title_vec = vectorizer.transform([titles[itemno]])\n",
    "    scores = []\n",
    "    for i in range(0, len(centroids)):\n",
    "        dist = np.dot(new_title_vec.toarray()[0], centroids[i].toarray()[0])\n",
    "        scores.extend([(dist, i)])\n",
    "    scores = sorted(scores,reverse=True) \n",
    "    return scores\n",
    "\n",
    "#this version works for an arbitrary text string\n",
    "def cosdistString(vectorizer, item, centroids):\n",
    "    new_title_vec = vectorizer.transform([item])\n",
    "    scores = []\n",
    "    for i in range(0, len(centroids)):\n",
    "        dist = np.dot(new_title_vec.toarray()[0], centroids[i].toarray()[0])\n",
    "        scores.extend([(dist, i)])\n",
    "    scores = sorted(scores,reverse=True) \n",
    "    return scores\n",
    "\n",
    "#returns list of tuples (distance, sitename, itemno, abstract for item)\n",
    "def compdist(new_title_vec, indexlist, X, titles):\n",
    "    similar = []\n",
    "    for i in indexlist:\n",
    "        if np.linalg.norm(X[i].toarray()) != 0.0:\n",
    "            #dist = np.linalg.norm((new_title_vec-X[i]).toarray())\n",
    "            dist = np.dot(new_title_vec.toarray()[0],X[i].toarray()[0])\n",
    "            similar.append((dist,i, titles[i]))\n",
    "    similar = sorted(similar,reverse=True) \n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#subject, loadset, basepath, supertopics =read_config(doc_service, \"all4\")\n",
    "    # for pushing to blob storage set basepath = \"\"\n",
    "basepath = \"/home/jovyan/work/celery/\"\n",
    "subject = \"phy\"\n",
    "cvectpath = basepath+\"count_vectorizer-\"+subject+\".p\"\n",
    "tfidftranpath = basepath+\"tfidf_transformer-\"+subject+\".p\"\n",
    "rfpath = basepath+\"random_forest-\"+subject+\".p\"\n",
    "namspath = basepath+\"supertopic_names-\"+subject+\".p\"\n",
    "GBpath = basepath+\"gradientboosting-\"+subject+\".p\"\n",
    "vectpath = basepath+\"vectorizer-\"+subject+\".p\"\n",
    "lsimodpath = basepath+\"lsimod-\"+subject+\".p\"\n",
    "lsiindpath = basepath+\"lsiind-\"+subject+\".p\"\n",
    "ldamodpath = basepath+\"ldamod-\"+subject+\".p\"\n",
    "ldaindpath = basepath+\"ldaind-\"+subject+\".p\"\n",
    "kmpath =  basepath+\"km-\"+subject+\".p\"\n",
    "ncentpath = basepath+\"ncent-\"+subject+\".p\"\n",
    "Xpath = basepath+\"Xtrain-\"+subject+\".p\"\n",
    "trainsetpath = basepath+\"Tset-\"+subject+\".p\"\n",
    "dictpath = basepath+\"Dict-\"+subject+\".p\"\n",
    "with open(cvectpath, 'rb') as f:\n",
    "    cvecb = f.read()\n",
    "with open(vectpath, 'rb') as f:\n",
    "    vecb = f.read()\n",
    "with open(tfidftranpath, 'rb') as f:\n",
    "    tranb = f.read()\n",
    "with open(namspath, 'rb') as f:\n",
    "    groupb = f.read()\n",
    "with open(rfpath, 'rb') as f:\n",
    "    rfbb = f.read()\n",
    "with open(lsimodpath, 'rb') as f:\n",
    "    lsimodb = f.read()\n",
    "with open(lsiindpath, 'rb') as f:\n",
    "    lsib = f.read()\n",
    "with open(ldamodpath, 'rb') as f:\n",
    "    ldamodb = f.read()\n",
    "with open(ldaindpath, 'rb') as f:\n",
    "    ldab = f.read()\n",
    "with open(kmpath, 'rb') as f:\n",
    "    kmpathb = f.read()\n",
    "with open(ncentpath, 'rb') as f:\n",
    "    ncentb = f.read()\n",
    "with open(Xpath, 'rb') as f:\n",
    "    Xb = f.read()\n",
    "with open(trainsetpath, 'rb') as f:\n",
    "    trainingb = f.read()\n",
    "with open(dictpath, 'rb') as f:\n",
    "    dictb = f.read()\n",
    "\n",
    "c_vectorizer = pickle.loads(str(cvecb))\n",
    "tfidf_transformer = pickle.loads(str(tranb))\n",
    "groupnames = pickle.loads(str(groupb))\n",
    "clfrf = pickle.loads(str(rfbb))\n",
    "vectorizer = pickle.loads(str(vecb))\n",
    "new_centroids = pickle.loads(str(ncentb))\n",
    "index_lsi = pickle.loads(str(lsib))\n",
    "lsi = pickle.loads(str(lsimodb))\n",
    "index_lda = pickle.loads(str(ldab))\n",
    "lda = pickle.loads(str(ldamodb))\n",
    "km = pickle.loads(str(kmpathb))\n",
    "Xtrain = pickle.loads(str(Xb))\n",
    "trainingSets = pickle.loads(str(trainingb))\n",
    "dictionary = pickle.loads(str(dictb))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "traininglist = makeInvertedTrainingList(trainingSets)\n",
    "traindocs = [tex[0] for tex in traininglist]\n",
    "trainlabel = [tex[1] for tex in traininglist]\n",
    "traintarget = [tex[2] for tex in traininglist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-detection \\nu Green Bank Telescope observations ultra-faint dwarf spheroidal galaxy Segue non-negligible halo magnetic field Milky Way bounds particle dark matter properties model galaxy Einasto dark matter profile synchrotron flux dark matter annihilation function magnetic field strength B diffusion coefficient D_0 particle mass m_\\chi different annihilation channels data disfavor annihilations m_\\chi sensitive b \\bar b channel fiducial B \\sim Segue proximity Milky Way models annihilation \\tau^+\\tau ^ m_\\chi intermediate value D_0 consistency data compelling limits WIMP annihilation \\mu^+\\mu ^ m_\\chi confidence D_0 Milky Way value B \n",
      "Astro\n"
     ]
    }
   ],
   "source": [
    "print traindocs[33]\n",
    "print trainlabel[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "statement = \"\"\n",
    "statement = traindocs[33]\n",
    "gl = makeguess(statement, km, vectorizer, lsi, dictionary, index_lsi, lda, index_lda, Xtrain, traindocs)\n",
    "\n",
    "print len(gl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0  523  Astro\n",
      "1.0  473  Astro\n",
      "1.0  108  Astro\n",
      "1.0  33  Astro\n",
      "0.333405502511  989  high eng\n",
      "0.276095396983  277  Astro\n",
      "0.276095396983  27  Astro\n",
      "0.276095396983  8  Astro\n",
      "0.244567490564  492  Astro\n",
      "0.244567490564  78  Astro\n",
      "0.195728070244  1208  high eng\n",
      "0.172697944777  1184  high eng\n",
      "0.172697944777  1131  high eng\n",
      "0.168100883865  1174  high eng\n",
      "0.164709975197  1040  high eng\n",
      "0.164709975197  1036  high eng\n",
      "0.164116095207  1169  high eng\n",
      "0.149216749855  1071  high eng\n",
      "0.149216749855  1046  high eng\n",
      "0.142829297232  351  Astro\n",
      "0.12957074053  449  Astro\n",
      "0.128809571458  71  Astro\n",
      "0.126327319563  1062  high eng\n",
      "0.126327319563  1001  high eng\n",
      "0.120333277516  867  GenRelQGr\n",
      "0.118888489006  478  Astro\n",
      "0.114896982065  245  Astro\n",
      "0.112429159679  748  GenRelQGr\n",
      "0.109364763421  463  Astro\n",
      "0.109364763421  403  Astro\n",
      "0.109364763421  292  Astro\n",
      "0.105051499835  879  GenRelQGr\n",
      "0.104909179466  1068  high eng\n",
      "0.104272141967  484  Astro\n",
      "0.103594373668  29  Astro\n",
      "0.0996341331268  1033  high eng\n",
      "0.0981179990895  362  Astro\n",
      "0.0933080811671  770  GenRelQGr\n",
      "0.0933080811671  727  GenRelQGr\n",
      "0.0930680598165  294  Astro\n",
      "0.0930680598165  162  Astro\n",
      "0.0906430881901  753  GenRelQGr\n",
      "0.0889004440345  352  Astro\n",
      "0.082370091132  656  GenRelQGr\n",
      "0.0810847615065  120  Astro\n",
      "0.0810847615065  56  Astro\n",
      "0.0802551011239  194  Astro\n",
      "0.0761034791034  652  GenRelQGr\n",
      "0.0724675647727  658  GenRelQGr\n",
      "0.0723310386045  521  Astro\n",
      "0.0723310386045  190  Astro\n",
      "0.0723310386045  68  Astro\n",
      "0.0720397438779  186  Astro\n",
      "0.071111196519  396  Astro\n",
      "0.0658515439355  549  Astro\n",
      "0.0646700168731  560  GenRelQGr\n",
      "0.0639405643827  271  Astro\n",
      "0.0635661436053  172  Astro\n",
      "0.0633088638418  807  GenRelQGr\n",
      "0.0512833816505  840  GenRelQGr\n",
      "0.0493230977244  529  Astro\n",
      "0.0486378685598  855  GenRelQGr\n",
      "0.0486378685598  718  GenRelQGr\n",
      "0.0486378685598  677  GenRelQGr\n",
      "0.0486378685598  575  GenRelQGr\n",
      "0.0479759495795  16  Astro\n",
      "0.0468905149929  402  Astro\n",
      "0.0468905149929  20  Astro\n",
      "0.0412608131185  782  GenRelQGr\n",
      "0.0412608131185  672  GenRelQGr\n",
      "0.0412608131185  622  GenRelQGr\n",
      "0.0403130606078  273  Astro\n",
      "0.0380247562298  1475  ed-physics\n",
      "0.0380247562298  1424  ed-physics\n",
      "0.0367254171127  756  GenRelQGr\n",
      "0.0367254171127  584  GenRelQGr\n",
      "0.03474745563  875  GenRelQGr\n",
      "0.03474745563  572  GenRelQGr\n",
      "0.0334087867799  378  Astro\n",
      "0.0332616406762  741  GenRelQGr\n",
      "0.0332616406762  626  GenRelQGr\n",
      "0.0314514777229  843  GenRelQGr\n",
      "0.0314514777229  568  GenRelQGr\n",
      "0.0306065936853  809  GenRelQGr\n",
      "0.0306065936853  789  GenRelQGr\n",
      "0.0306065936853  574  GenRelQGr\n",
      "0.0298974467269  819  GenRelQGr\n",
      "0.029594641767  1114  high eng\n",
      "0.029594641767  1039  high eng\n",
      "0.0288333914644  1030  high eng\n",
      "0.0272613974549  50  Astro\n",
      "0.0263759051866  1269  MathPh\n",
      "0.0263759051866  1210  MathPh\n",
      "0.0231626805342  738  GenRelQGr\n",
      "0.0223173871363  545  Astro\n",
      "0.0223173871363  358  Astro\n",
      "0.0223173871363  11  Astro\n",
      "0.0185042547333  537  Astro\n",
      "0.0185042547333  339  Astro\n",
      "0.0162104977263  460  Astro\n",
      "0.0162104977263  434  Astro\n",
      "0.0162104977263  314  Astro\n",
      "0.0150424749695  298  Astro\n",
      "0.0149522026423  480  Astro\n",
      "0.0149522026423  398  Astro\n",
      "0.014222838684  481  Astro\n",
      "0.0141506964178  243  Astro\n",
      "0.0141506964178  144  Astro\n",
      "0.00371201587867  493  Astro\n",
      "0.00371201587867  461  Astro\n",
      "0.00371201587867  204  Astro\n",
      "0.00371201587867  173  Astro\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(gl)):\n",
    "    print str(gl[i][0])+ \"  \"+ str(gl[i][1])+ \"  \"+trainlabel[gl[i][1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now use random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stmt = \"Einsteins general theory of relativity explains gravity as curvature in spacetime\"\n",
    "#stmt = traindocs[33]\n",
    "X_new_counts = c_vectorizer.transform([stmt])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predictedrf = clfrf.predict(X_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenRelQGr\n"
     ]
    }
   ],
   "source": [
    "print groupnames[predictedrf[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#now do centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "item = \"genome dna rna gene sequence alignment express\"\n",
    "item = traindocs[33]\n",
    "z = cosdistString(vectorizer, item, new_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.20095832754718015, 0), (0.12056768576374675, 3), (0.089435731081336012, 1), (0.061377081514750839, 2)]\n"
     ]
    }
   ],
   "source": [
    "print z[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astro\n"
     ]
    }
   ],
   "source": [
    "print groupnames[z[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bigpredict(statement, km, vectorizer, lsi, dictionary, index_lsi, lda, index_lda, \n",
    "               Xtrain, traindocs, c_vectorizer, clfrf, tfidf_transformer, new_centroids  ):\n",
    "    \n",
    "    bestof3l = makeguess(statement, km, vectorizer, lsi, dictionary, index_lsi, \n",
    "                        lda, index_lda, Xtrain, traindocs)\n",
    "    if len(bestof3l)> 0:\n",
    "        best = trainlabel[bestof3l[0][1]]\n",
    "    else:\n",
    "        best = \"?\"\n",
    "    if len(bestof3l) > 1:\n",
    "        nextb = trainlabel[bestof3l[1][1]]\n",
    "    else:\n",
    "        nextb = \"?\"\n",
    "    \n",
    "    X_new_counts = c_vectorizer.transform([statement])\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predictedrf = clfrf.predict(X_new_tfidf)\n",
    "    rf = groupnames[predictedrf[0]]\n",
    "    \n",
    "    z = cosdistString(vectorizer, statement, new_centroids)\n",
    "    cent = groupnames[z[0][1]]\n",
    "    \n",
    "    return rf, best, nextb, cent\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n"
     ]
    }
   ],
   "source": [
    "print len(traindocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astro\n",
      "rf=Astro best=Astro nextb=Astro cent=Astro\n"
     ]
    }
   ],
   "source": [
    "statement = traindocs[33]\n",
    "print trainlabel[33]\n",
    "rf, best, nextb, cent = bigpredict(statement, km, vectorizer, lsi, dictionary, index_lsi, \n",
    "    lda, index_lda, Xtrain, traindocs, c_vectorizer, clfrf, tfidf_transformer, new_centroids  )\n",
    "print \"rf=\"+rf+\" best=\"+best+\" nextb=\"+nextb+\" cent=\"+cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topl = []\n",
    "for top in supertopics:\n",
    "    s = \" \"+top[0].lower()+\". \"\n",
    "    for x in top[1]:\n",
    "        s = s + x + \" \"\n",
    "    topl.extend([(top[0], s)])\n",
    "topdict = {x[0]:x[1] for x in topl}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findTopic(topdict, groupnames, found):\n",
    "    for nm in groupnames:\n",
    "        if topdict[nm].find(found)> 0:\n",
    "            return nm\n",
    "    return \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'astro-ph.CO',\n",
       " u'astro-ph.EP',\n",
       " u'astro-ph.GA',\n",
       " u'astro-ph.HE',\n",
       " u'astro-ph.IM',\n",
       " u'astro-ph.SR',\n",
       " u'cond-mat.dis-nn',\n",
       " u'cond-mat.mes-hall',\n",
       " u'cond-mat.mtrl-sci',\n",
       " u'cond-mat.other',\n",
       " u'cond-mat.quant-gas',\n",
       " u'cond-mat.soft',\n",
       " u'cond-mat.stat-mech',\n",
       " u'cond-mat.str-el',\n",
       " u'cond-mat.supr-con',\n",
       " u'nucl-th',\n",
       " u'quant-ph',\n",
       " u'gr-qc',\n",
       " u'hep-ex',\n",
       " u'hep-ph',\n",
       " u'hep-th',\n",
       " u'math-ph',\n",
       " u'physics.acc-ph',\n",
       " u'physics.ao-ph',\n",
       " u'physics.atm-clus',\n",
       " u'physics.atom-ph',\n",
       " u'physics.bio-ph',\n",
       " u'physics.chem-ph',\n",
       " u'physics.class-ph',\n",
       " u'physics.comp-ph',\n",
       " u'physics.data-an',\n",
       " u'physics.ed-ph',\n",
       " u'physics.flu-dyn',\n",
       " u'physics.gen-ph',\n",
       " u'physics.geo-ph',\n",
       " u'physics.hist-ph',\n",
       " u'physics.ins-det',\n",
       " u'physics.med-ph',\n",
       " u'physics.optics',\n",
       " u'physics.plasm-ph',\n",
       " u'physics.pop-ph',\n",
       " u'physics.soc-ph',\n",
       " u'physics.space-ph']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data2(titles, sitenames, disp_title, loadset):\n",
    "    s = set(loadset)\n",
    "    ti = []\n",
    "    si = []\n",
    "    di = []\n",
    "    for i in range(len(titles)):\n",
    "        if sitenames[i] in s:\n",
    "            ti.append(titles[i])\n",
    "            si.append(sitenames[i])\n",
    "            di.append(disp_title[i])\n",
    "    return ti, si, di    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "titles, sitenames, disp_title = load_data2(titles, sitenames, disp_title, loadset)\n",
    "print \"data loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct rate = 88.1968859869\n",
      "Astro \t= 97.94 \t rf win = 96.70 \t best win = 93.00\t cent win 88.75\n",
      "GenRelQGr \t= 93.69 \t rf win = 80.40 \t best win = 85.36\t cent win 85.13\n",
      "Cond Matt \t= 75.17 \t rf win = 41.84 \t best win = 75.17\t cent win 68.08\n",
      "high eng \t= 77.62 \t rf win = 46.10 \t best win = 73.89\t cent win 57.62\n",
      "MathPh \t= 74.46 \t rf win = 50.35 \t best win = 70.21\t cent win 65.24\n",
      "quantum \t= 65.82 \t rf win = 48.10 \t best win = 65.82\t cent win 63.29\n",
      "ed-physic \t= 82.71 \t rf win = 64.81 \t best win = 82.09\t cent win 74.69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outTable = {gname:0 for gname in groupnames}\n",
    "outCount = {gname:0 for gname in groupnames}\n",
    "rfWin = {gname:0 for gname in groupnames}\n",
    "bestWin = {gname:0 for gname in groupnames}\n",
    "nextWin = {gname:0 for gname in groupnames}\n",
    "centWin = {gname:0 for gname in groupnames}\n",
    "falseposBes = {gname:0 for gname in groupnames}\n",
    "falseposRf = {gname:0 for gname in groupnames}\n",
    "bothAgr = {gname:0 for gname in groupnames}\n",
    "listTable = {gname:[] for gname in groupnames}\n",
    "num = 0.\n",
    "correct = 0.\n",
    "dataIn = []\n",
    "ans = []\n",
    "for i in range(0, len(titles)):\n",
    "    statement = titles[i]\n",
    "    rf, best, nextb, cent = bigpredict(statement, km, vectorizer, lsi, dictionary, index_lsi, \n",
    "    lda, index_lda, Xtrain, traindocs, c_vectorizer, clfrf, tfidf_transformer, new_centroids  )\n",
    "    tup = (rf,best,nextb, cent, i, disp_title[i], titles[i], sitenames[i])\n",
    "    #print \"rf=\"+rf[0:5]+\" best=\"+best[0:5]+\" nextb=\"+nextb[0:5]+\" cent=\"+cent[0:5]\n",
    "    dt = disp_title[i]\n",
    "    j = dt.find(\"[\")\n",
    "    if j > 0:\n",
    "        jj = dt.find(\"]\")\n",
    "        q = dt[j+1:jj]\n",
    "        #print q\n",
    "    else:\n",
    "        q = \"none\"\n",
    "    if q != \"none\":\n",
    "        p = findTopic(topdict, groupnames, q)\n",
    "        #print \"p =\"+p\n",
    "        if  p != \"none\":\n",
    "            num = num+1.0\n",
    "            outCount[p] = outCount[p]+1\n",
    "            if p == rf or p == best:\n",
    "                correct = correct+1\n",
    "                outTable[p] = outTable[p]+1\n",
    "                #prstr = \"best=\"+best[0:4] + \"\\tsecond=\"+nextb[0:4]+ \"\\trb=\"+t[0:4]+\"\\tsec=\"+sec[0:4]\n",
    "                #print prstr + \"\\tfound = \"+ findTopic(topdict, groupnames, q)[0:4] +\" \"+str(i)\n",
    "                if p == best:\n",
    "                    bestWin[p] = bestWin[p]+1\n",
    "                if p == rf:\n",
    "                    rfWin[p] = rfWin[p]+1\n",
    "                if p == cent:\n",
    "                    centWin[p] = centWin[p]+ 1 \n",
    "            #dataIn.extend([(nameIndex[best],nameIndex[nextb],nameIndex[t],nameIndex[sec])])\n",
    "            #ans.extend([nameIndex[findTopic(topdict, groupnames, q)]])\n",
    "            if( p == best and p == rf):\n",
    "                bothAgr[p] = bothAgr[p]+ 1\n",
    "            if (p != best):\n",
    "                falseposBes[best] = falseposBes[best]+1 \n",
    "            if (p != rf):\n",
    "                falseposRf[rf] = falseposRf[rf]+1 \n",
    "            # this will only work for the labled data.  save it under \"true\" category\n",
    "            listTable[p].extend([tup])\n",
    "    # the following is the way it should be done\n",
    "    #listTable[best].extend([tup])\n",
    "    #if best != nextb:\n",
    "        #listTable[nextb].extend([tup])\n",
    "print \"correct rate = \"+ str(100.0*correct/num)\n",
    "for nam in groupnames:\n",
    "    if outCount[nam]> 0:\n",
    "        ans = nam[0:9] + \" \\t= \"+ str(100.0*outTable[nam]/outCount[nam])[0:5]\n",
    "        #ans = ans+  \" \\tboth agree =\"+str(100.0*bothAgr[nam]/outCount[nam])[0:5]\n",
    "        ans = ans+ \" \\t rf win = \"+str(100.0*rfWin[nam]/outCount[nam])[0:5]\n",
    "        ans = ans+ \" \\t best win = \"+str(100.0*bestWin[nam]/outCount[nam])[0:5]\n",
    "        ans = ans + \"\\t cent win \" +str(100.0*centWin[nam]/outCount[nam])[0:5]\n",
    "        #ans = ans+ \" \\t false pos rf = \"+str(100.0*falseposRf[nam]/num)[0:5]\n",
    "        #ans = ans+ \" \\t false pos best = \"+str(100.0*falseposBes[nam]/num)[0:5]\n",
    "        print ans\n",
    "    #pickled = pickle.dumps(listTable[nam])\n",
    "    #path = \"labeled_dump_\"+subj+\"_subtopic_\"+nam+\".p\"\n",
    "    #path = \"dump_\"+subj+\"_subtopic_\"+nam+\".p\"\n",
    "    #doc_service.put_block_blob_from_bytes(\"sciml\", path, pickled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
